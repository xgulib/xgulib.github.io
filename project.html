<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Xiao Gu" />


<title>BST 260 Final Project</title>

<script src="site_libs/header-attrs-2.18/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">BST260 Final Project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="project.html">
    <span class="fa fa-stethoscope"></span>
     
    The Project
  </a>
</li>
<li>
  <a href="project_pdf.html">
    <span class="fa fa-file-pdf-o"></span>
     
    Download Report
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:xgu@g.harvard.edu">
    <span class="fa fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
<li>
  <a href="https://github.com/xgulib">
    <span class="fa fa-github"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">BST 260 Final Project</h1>
<h4 class="author">Xiao Gu</h4>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Ischemic heart disease (IHD) has been identified as a leading cause
of death globally (Ref). Compelling evidence showed that lifestyle
changes could be effective strategies for secondary preventions of IHD
(Ref). Therefore, to reduce the burden of IHD mortality, an efficient
tool for IHD screening and early diagnosis is warranted. A machine
learning algorithm that is developed with serum metabolites,
cardiometabolic biomarkers, and self-reported phenotypic data is
promising in simplifying the process and reduce the cost of IHD
screening/diagnosis. IHD status could be accurately detected with a
simple blood draw and metabolomic profiling. In this project, I aim to
develop such an algorithm using data from a European population.</p>
<p>I will use data from the MetaCardis consortium that recruited
participants aged 18-75 years from Denmark, France, and Germany (Ref).
The data was published early this year as the supplementary material of
an article on Nature Medicine (Ref). The original study included 372
individuals with IHD. These IHD cases were further classified into acute
coronary syndrome (n = 112), chronic ischemic heart disease, (n = 158),
and heart failure (n = 102). With a case-control design, the study also
included 3 groups of controls matched on varies factors. The raw data
includes 1,882 observations including repeated records with the same
participant ID but different case-control status.</p>
<p>For this project, I will use records from the 372 IHD cases and 372
controls matched on type 2 diabetes (T2D) status and body mass index
(BMI). I will also extract data for age, gender, fasting plasma
triglycerides, adiponectin, and CRP, systolic and diastolic blood
pressure, left ventricular ejection fraction, physical activity level,
and 1,513 log-transformed values of serum metabolites.</p>
<div id="exploratory-data-analysis" class="section level3">
<h3>Exploratory data analysis</h3>
<p>After reading in the data, I first filtered the observations to keep
the IHD cases and their controls matched by T2D status and BMI. I then
merged metabolites data with cardiometabolic biomarkers and
self-reported phenotypic data to create a <code>main</code> dataset with
744 rows and 1522 columns. I noticed that several participants do not
have any metabolites data, and therefore, need to be removed.
Additionally, around 30% of participants had missing values for left
ventricular ejection fraction and physical activity level. Many machine
learning techniques could not be implemented with that many missing and
it would also not be appropriate to replace the missings with any
arbitrarily selected value. So I removed these two potential predictors
from my analyses. Finally, for variables with less than 10% missing
data, I replaced the missing values with the median of the non-missing
data. The cleaned <code>main</code> dataset had 603 rows and 1522
columns.</p>
<p>I then preprocessed the data to remove non-informative predictors
with near-zero variances. Given that I planned to train as least one of
my algorithms with regression, it would be better to have more
predictors normally distributed so that model efficiencies could be
improved. I tested the normality of each predictor with Shapiro-wilks
Test and summarized the p-values. I found that only 101 predictors are
normally distributed. It is also note-worthy that the metabolite values
from the raw data were all log-transformed. Obviously, log-tranformation
did not normalize the distributions successfully. So I transformed all
metabolite values back to the original scale and used rank-based inverse
normal transformation (INT) to normalized the distributions instead. As
examples, histograms showing the distributions of oleoylcarnitine
(C18:1) and S-methylcysteine sulfoxide before and after the
transformation were shown. I ended up having 840 predictors normalized
successfully.</p>
</div>
<div id="methodologies-to-use" class="section level3">
<h3>Methodologies to use</h3>
<p>The outcome that my algorithm aimed to predict is the binary IHD
status (non-case = 0, case = 1). Considering that I had 1422 predictors,
I would use principle component analysis (PCA) to reduce dimensions. I
would keep principle components that account for at least 70% of
variability as new predictors, and train a model with logistic
regression, and a model with K-nearest neighbor (KNN). Given that the
principle components are hard to interpret and algorithms developed
based on PCA could be difficult to implement, I would train another KNN
model with all 1422 predictors instead. Random forest would be the 4th
training method I would use. Finally, I will use ensemble to combine the
results of all four algorithms. For all algorithms, I would evaluate the
overall accuracy, sensitivity, specificity, <span
class="math inline">\(F_1\)</span> score, and ROC curve. I would use a
<span class="math inline">\(\beta\)</span> of 2 to calculate the <span
class="math inline">\(F_1\)</span> score because higher sensitivity is
more important than high specificity when predicting disease. In other
words, false positive will be less costly than false negative in this
scenario. I would also use cross-validation and bootstrapping to tune
the model parameters.</p>
</div>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>For all the model training and fitting, I partitioned the
<code>main</code> dataset, which includes IHD case status and all
predictors, into a training and a testing dataset. Matrices for
predictors and cases were also created. I then train and assess the
models with the following 4 approaches: 1) PCA + logistic regression; 2)
PCA + KNN; 3) KNN; 4) Random forest.</p>
<div id="pca-logistic-regression" class="section level3">
<h3>PCA + logistic regression</h3>
<p>The PCA in the training set generated 483 principle components (PC)
from 1422 predictors including age, gender, fasting plasma
triglycerides, adiponectin, and CRP, systolic and diastolic blood
pressure, and 1415 inverse normal transformed serum metabolites. After
evaluating the proportion of variance explained by each PC, I selected
the first 69 PCs that account for 70% of the total variance as new
predictors. I fitted a logistic regression with IHD cases as the
dependent variable and the 69 PCs as the independent variables. For the
logistic regression, there was no model parameter to tune. To make
predictions in the testing set, I used the PC rotations to transform all
1422 predictors in the testing set into 483 PCs and kept the first 69
PCs. The logistic regression estimates were then used to predict the
probability of having IHD cases in the testing set. Participants with a
predicted probability of having IHD over 0.5 were defined as predicted
IHD cases.</p>
<p>The overall accuracy of my predicted IHD cases from the logistic
regression was 0.875 with a 95% confidence interval of (0.802, 0.928).
This algorithm had a sensitivity of 0.892, a specificity of 0.854, and
an <span class="math inline">\(F_1\)</span> score of 0.890. I also
plotted the ROC and observed an area under the curve (AUC) of 0.946,
which was very high.</p>
</div>
<div id="pca-knn" class="section level3">
<h3>PCA + KNN</h3>
<p>I then used KNN to train the model with the 69 PCs as predictors. To
select the parameter K that maximize the accuracy, I used 10-fold
cross-validation with bootstrapping as the resampling scheme. Given that
I have already reduced the dimension to 69 and we only have 603
observations, I did not worry much about the computation time of using
10-fold cross-validation. I fitted the model with K values from 2 to 100
with 20 as the increment. After plotting the model accuracy under
different K values, I was not able to identify a clear optimized K given
that the curve of accuracy did not go down within the specified K range.
Therefore, I fitted the model with K values from 5 to 150 with 10 as the
increment instead. I identified 75 as the K for the maximum model
accuracy and fitted the model again with this value. The fitted KNN
model was then used to predict the IHD cases in the testing set.</p>
<p>Using the combination of PCA and KNN, the overall accuracy of my
predicted IHD cases was 0.842 with a 95% confidence interval of (0.764,
0.902). Comparing to the algorithm developed with PCA and logistic
regression, this algorithm had a higher sensitivity of 0.923, a lower
specificity of 0.746, and a higher <span
class="math inline">\(F_1\)</span> score of 0.898. I plotted the ROC and
observed an AUC of 0.889.</p>
</div>
<div id="knn" class="section level3">
<h3>KNN</h3>
<p>The previous two algorithms developed based on selected PCs already
performed well in predicting IHD cases. However, people who wants to
implement these two algorithms have to use the PCA rotations to
transform their data first. That could increase the burden of using
these algorithms, particularly in clinical settings. Also, the PCs no
longer have biological meaning, and therefore, could be difficult to
interpret. With these concerns, I developed another KNN-based algorithm
with the 1422 predictors, including 1415 serum metabolites.</p>
<p>Given that the sample size of my study is not large, I used 10-fold
cross-validation with bootstrapping as the resampling scheme to select
the parameter K again. I found 65 as the K that maximize the model
accuracy after fitting the model with K values from 5 to 150 with 10 as
the increment. I then fitted the model in the training set and predicted
the IHD cases in the testing set. The overall accuracy of my predicted
IHD cases was 0.800 with a 95% confidence interval of (0.717, 0.868).
Comparing to the algorithm developed with PCA and KNN, this KNN
algorithm had a slightly higher sensitivity of 0.939. But the
specificity dropped to 0.636. The <span
class="math inline">\(F_1\)</span> score was 0.894. I plotted the ROC
and observed an AUC of 0.897.</p>
</div>
<div id="random-forest" class="section level3">
<h3>Random forest</h3>
<p>The last approach I used to train my model is random forest. It is
more computationally intensive because predictors have to be randomly
selected using bootstrapping to predict a single tree. To stabilize
accuracy, hundreds of trees might need to be predicted. Also, I have to
change the number of predictors being sampled at each bootstrap
iteration to find the one that maximize the accuracy. Therefore, I
started training the model with 15 trees and tuning the number of
predictors to be sampled between 10 and 1000 with 100 as the increment.
I implemented a 5-fold cross-validation. The plot of error against
number of trees showed that the accuracy improves as we add more trees
and stabilized at around 100 trees. In my second attempt, I changed the
number of trees to be predicted to 100. The plot of accuracy by the
number of randomly sampled predictors did show a maximum point. However,
it seems that the range of 10 to 1000 predictors was too large. So I
further tuned the number of predictors to be sampled from 10 to 500 with
20 as the increment. It turned out that randomly sample 150 predictors
and predict 100 trees maximized and stabilized the accuracy of model
prediction.</p>
<p>The overall accuracy of my predicted IHD cases from the random forest
model was 0.900 with a 95% confidence interval of (0.832, 0.947). This
algorithm had a high sensitivity of 0.939, a high specificity of 0.855,
an high <span class="math inline">\(F_1\)</span> score of 0.927, and a
high AUC of 0.958.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this project, I aimed to develop an algorithm that uses serum
metabolites, cardiometabolic biomarkers, and self-reported phenotypic
data to predict ischemic heart disease (IHD) status in a European
population. I obtained my data from a paper published early this year on
Nature Medicine (Ref). For data preprocessing, I removed observations
with missing metabolites measures, and predictors with at least around
30% of missing data. For predictors with small amount of missing data, I
replaced the missing values with median values. Additionally, predictors
with near-zero variance were also excluded. I used 4 approaches to train
my model. The first two approaches used PCA to reduce dimension from
1422 predictors. A logistic regression and a KNN model were trained and
fitted with the selected 69 PCs. The 3rd approach was also based on KNN
but fitted the model with the 1422 predictors. The last approach used
random forest to develop the algorithm. I summarized the sensitivity,
specificity, overall accuracy, <span class="math inline">\(F_1\)</span>
score, and AUC of all models in a table. I also conducted an ensemble to
combine results from the KNN model and random forest model and showed
the performance at the end of the table. ROC curves were plotted on the
same figure for comparison.</p>
<p>According to the table, the two models developed with PCs had lower
sensitivity than those trained with all predictors. The KNN and random
forest models both had a very high sensitivity of 0.938. The two models
developed with KNN had lower specificity than the others. The random
forest model also had a relatively high specificity of 0.855. The KNN
model biologically meaningful predictors had the lowest overall accuracy
while the random forest model had the highest overall accuracy. The
models with PCs as predictors and used logistic regression for fitting
had an overall accuracy of 0.875 while the model using PCs and KNN had
an accuracy of 0.842. When evaluating with <span
class="math inline">\(F_1\)</span> score, the random forest model
performed the best while the rest three models performed similarly.
Finally, the random forest model had the highest AUC, followed by the
PCA + KNN model. It is interesting that ensemble of KNN and random
forest did not further improve the model performance. In conclusion, the
algorithm developed with random forest performed the best in all
measures.</p>
<p>Successful?</p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<pre class="r"><code>library(tidyverse)
library(readxl)
library(caret)
library(RNOmni)
library(pROC)
library(randomForest) 
library(kableExtra)

#Read in
meta &lt;- read_excel(&quot;/Users/xgu/Documents/Harvard/Fall 2022/BST260/bst260project/41591_2022_1688_MOESM3_ESM.xlsx&quot;, 
                          sheet = 13, skip = 1, na = &quot;NA&quot;, col_types = &quot;guess&quot;)
demo &lt;- read_excel(&quot;/Users/xgu/Documents/Harvard/Fall 2022/BST260/bst260project/41591_2022_1688_MOESM3_ESM.xlsx&quot;, 
                   sheet = 10, skip = 1, na = &quot;NA&quot;, col_types = c(&quot;text&quot;, &quot;text&quot;, rep(&quot;numeric&quot;, 22), &quot;text&quot;, &quot;text&quot;, &quot;text&quot;))

#Selection and filtering
demo_new &lt;- demo %&gt;%
  filter(Status %in% c(&quot;IHD372&quot;, &quot;MMC372&quot;)) %&gt;%
  mutate(case = case_when(Status == &quot;MMC372&quot; ~ 0, TRUE ~ 1),
         Gender = case_when(Gender == &quot;Male&quot; ~ 1, TRUE ~ 0)) %&gt;%
  rename(age = &quot;Age (years)&quot;, tag = &quot;Fasting plasma triglycerides (mmol/L)&quot;, 
         adiponectin = &quot;Fasting plasma adiponectin (mg/L)&quot;, crp = &quot;Fasting plasma CRP (mg/L)&quot;,
         sbp = &quot;Systolic blood pressure (mmHg)&quot;, dbp = &quot;Diastolic blood pressure (mmHg)&quot;, 
         lvef = &quot;Left ventricular ejection fraction (%)&quot;, act = &quot;Physical activity (h/week)&quot;) %&gt;%
  select(ID, case, age, tag, adiponectin, crp, sbp, dbp, Gender, lvef, act)
  
meta_new &lt;- meta %&gt;%
  filter(Status %in% c(&quot;IHD372&quot;, &quot;MMC372&quot;)) %&gt;%
  select(-c(Status))

#Merge
main &lt;- demo_new %&gt;%
  left_join(meta_new, by = &quot;ID&quot;)
head(main)</code></pre>
<pre class="r"><code>#Check missing
pctmiss &lt;- function(x){
  pctmiss &lt;- sum(is.na(x))/length(x)
  return(pctmiss)
}
miss &lt;- as.data.frame(sapply(main, pctmiss))
head(miss)

main &lt;- main %&gt;% 
  select(-c(&quot;lvef&quot;, &quot;act&quot;)) %&gt;% 
  filter(acetate != &quot;NA&quot;, spermidine != &quot;NA&quot;) %&gt;%
  mutate(tag = case_when(is.na(tag) ~ median(tag, na.rm = TRUE), TRUE ~ tag),
         adiponectin = case_when(is.na(adiponectin) ~ median(adiponectin, na.rm = TRUE), TRUE ~ adiponectin),
         crp = case_when(is.na(crp) ~ median(crp, na.rm = TRUE), TRUE ~ crp),
         sbp = case_when(is.na(sbp) ~ median(sbp, na.rm = TRUE), TRUE ~ sbp),
         dbp = case_when(is.na(dbp) ~ median(dbp, na.rm = TRUE), TRUE ~ dbp))
head(main)</code></pre>
<pre class="r"><code>var &lt;- main %&gt;% select(-c(&quot;ID&quot;, &quot;case&quot;))

#Preprocessing
nzv &lt;- nearZeroVar(var)
col_index &lt;- setdiff(1:ncol(var), nzv)
length(col_index)

var_proc &lt;- var[,col_index]

#check normality
normality &lt;- data.frame()
for (i in 1:length(colnames(var_proc))){
  normality[i, 1] &lt;- colnames(var_proc)[i]
  normality[i, 2] &lt;- shapiro.test(pull(var_proc[,i]))$p.value
  colnames(normality) &lt;- c(&quot;metabolites&quot;, &quot;shapiro.p&quot;)
}
table(ifelse(normality$shapiro.p &gt;0.05, 1, 0))
#which(normality$shapiro.p &gt; 0.05)

#Log transformation not work
m &lt;- as.matrix(var_proc[,8:1422])
exp_m &lt;- exp(m)
var_proc_exp &lt;- cbind(var_proc[,1:7], as.data.frame(exp_m))

var_proc_int &lt;- as.data.frame(sapply(var_proc_exp, RankNorm))

#check normality again!
normality_int &lt;- data.frame()
for (i in 1:length(colnames(tibble(var_proc_int)))){
  normality_int[i, 1] &lt;- colnames(tibble(var_proc_int))[i]
  normality_int[i, 2] &lt;- shapiro.test(pull(tibble(var_proc_int)[,i]))$p.value
  colnames(normality_int) &lt;- c(&quot;metabolites&quot;, &quot;shapiro.p&quot;)
}
table(ifelse(normality_int$shapiro.p &gt;0.05, 1, 0))
#which(normality_int$shapiro.p &gt; 0.05)

hist(var_proc_exp$`oleoylcarnitine (C18:1)`, main = &quot;Histogram of oleoylcarnitine (C18:1)&quot;, xlab = &quot;Oleoylcarnitine (C18:1)&quot;)
hist(var_proc_int$`oleoylcarnitine (C18:1)`, main = &quot;Histogram of INT-transformed oleoylcarnitine (C18:1)&quot;, xlab = &quot;INT(Oleoylcarnitine (C18:1))&quot;)
hist(var_proc_exp$`S-methylcysteine sulfoxide`, main = &quot;Histogram of S-methylcysteine sulfoxide&quot;, xlab = &quot;S-methylcysteine sulfoxide&quot;)
hist(var_proc_int$`S-methylcysteine sulfoxide`, main = &quot;Histogram of INT-transformed S-methylcysteine sulfoxide&quot;, xlab = &quot;INT(S-methylcysteine sulfoxide)&quot;)   </code></pre>
<pre class="r"><code>#Split data
set.seed(34324)
main_new &lt;- cbind(main[,1:2], var_proc_int)
train_index &lt;- createDataPartition(main_new$case, times = 1, p = 0.8, list = FALSE)
train_set &lt;- main_new[train_index,]
test_set &lt;- main_new[-train_index,]

#X and Y matrix
x_train &lt;- as.matrix(train_set[,3:1424])
y_train &lt;- factor(train_set$case)
x_test &lt;- as.matrix(test_set[,3:1424])
y_test &lt;- factor(test_set$case)</code></pre>
<pre class="r"><code>#######PCA
col_means &lt;- colMeans(x_train)

pca &lt;- prcomp(x_train)
s_pca_3 &lt;- summary(pca)$importance[3,] ##69 pc
head(s_pca_3, 69)

pc &lt;- 69
x_train_pc &lt;- pca$x[,1:pc] </code></pre>
<pre class="r"><code>#####PCA + glm
glm_tmp &lt;- as.data.frame(cbind(train_set$case, x_train_pc))
glm_tmp &lt;- glm_tmp %&gt;% rename(case = V1)
fit_glm &lt;- glm(case ~., data = glm_tmp, family = &quot;binomial&quot;)

x_test_pc_pre &lt;- sweep(x_test,2,col_means) %*% pca$rotation 
x_test_pc &lt;- x_test_pc_pre[,1:pc]
y_prob &lt;- predict(fit_glm, as.data.frame(x_test_pc), type = &quot;response&quot;)
y_pred_glm &lt;- factor(ifelse(y_prob &gt; 0.5, 1, 0))
confusionMatrix(y_pred_glm, y_test)$overall[[&quot;Accuracy&quot;]] 
F_meas(y_pred_glm, y_test, beta = 2)

#ROC
roc_glm &lt;- roc(as.factor(test_set$case), y_prob)
plot(roc_glm, print.thres=&quot;best&quot;, type = &quot;line&quot;, print.auc = TRUE, legacy.axes = TRUE,
     grid = TRUE, ylim = c(0,1), xlim = c(1,0), col = &quot;Red&quot;, main = &quot;ROC for PCA + GLM&quot;) 


#######PCA + KNN
set.seed(5436)
b &lt;- 10
control_pca &lt;- trainControl(method = &quot;cv&quot;, number = b, p = .9) 
train_pcaknn &lt;- train(x_train_pc, y_train,  
                   method = &quot;knn&quot;,  
                   tuneGrid = data.frame(k = seq(2,100,20)), #
                   trControl = control_pca) 
ggplot(train_pcaknn, highlight = TRUE)

set.seed(5432)
b &lt;- 10
control_pca &lt;- trainControl(method = &quot;cv&quot;, number = b, p = .9) 
train_pcaknn2 &lt;- train(x_train_pc, y_train,  
                   method = &quot;knn&quot;,  
                   tuneGrid = data.frame(k = seq(5,150,10)), #
                   trControl = control_pca) 
ggplot(train_pcaknn2, highlight = TRUE)

train_pcaknn2$bestTune 
train_pcaknn2$results$Accuracy
fit_pcaknn &lt;- knn3(x_train_pc, y_train, k = train_pcaknn2$bestTune$k) 

y_pred_pcaknn &lt;- predict(fit_pcaknn, x_test_pc, type = &quot;class&quot;)
y_pred_pcaknn_p &lt;- predict(fit_pcaknn, x_test_pc, type = &quot;prob&quot;)
confusionMatrix(y_pred_pcaknn, y_test)$overall[&quot;Accuracy&quot;]
F_meas(y_pred_pcaknn, y_test, beta = 2)

#ROC
roc_pcaknn &lt;- roc(as.factor(test_set$case), y_pred_pcaknn_p[, 2])
plot(roc_pcaknn, print.thres=&quot;best&quot;, type = &quot;line&quot;, print.auc = TRUE, grid = TRUE, ylim = c(0,1), col = &quot;Red&quot;)</code></pre>
<pre class="r"><code>#####KNN
set.seed(3245)
b &lt;- 10
control_knn &lt;- trainControl(method = &quot;cv&quot;, number = b, p = .9) 
train_knn &lt;- train(x_train, y_train,  
                   method = &quot;knn&quot;,  
                   tuneGrid = data.frame(k = seq(5,150,10)), 
                   trControl = control_knn) 
ggplot(train_knn, highlight = TRUE)
train_knn$bestTune 
train_knn$results$Accuracy
fit_knn &lt;- knn3(x_train, y_train, k = train_knn$bestTune$k) 

y_pred_knn &lt;- predict(fit_knn, x_test, type = &quot;class&quot;)
y_pred_knn_p &lt;- predict(fit_knn, x_test, type = &quot;prob&quot;)
confusionMatrix(y_pred_knn, y_test)
confusionMatrix(y_pred_knn, y_test)$overall[&quot;Accuracy&quot;]
F_meas(y_pred_knn, y_test, beta = 2)

#ROC
roc_knn &lt;- roc(as.factor(test_set$case), y_pred_knn_p[, 2])
plot(roc_knn, print.thres=&quot;best&quot;, type = &quot;line&quot;, print.auc = TRUE, grid = TRUE, ylim = c(0,1))</code></pre>
<pre class="r"><code>##########Random forest
set.seed(4536)
b &lt;- 5
control_rf &lt;- trainControl(method=&quot;cv&quot;, number = b, p = .9) 
train_rf &lt;-  train(x_train, y_train,  
                   method = &quot;rf&quot;,  
                   ntree = 15,
                   trControl = control_rf, 
                   tuneGrid = data.frame(mtry = seq(10, 1000, 100))) 
ggplot(train_rf, highlight = TRUE)
train_rf$bestTune
train_rf$results$Accuracy
fit_rf &lt;- randomForest(x_train, y_train,  
                       mtry = train_rf$bestTune$mtry,
                       minNode = 10) 
plot(fit_rf) 


set.seed(6543)
b &lt;- 5
control_rf &lt;- trainControl(method=&quot;cv&quot;, number = b, p = .9) 
train_rf2 &lt;-  train(x_train, y_train,  
                   method = &quot;rf&quot;,  
                   ntree = 100,
                   trControl = control_rf, 
                   tuneGrid = data.frame(mtry = seq(10, 1000, 100))) 
ggplot(train_rf2, highlight = TRUE)
train_rf2$bestTune
train_rf2$results$Accuracy
fit_rf2 &lt;- randomForest(x_train, y_train,  
                       mtry = train_rf2$bestTune$mtry,
                       minNode = 10) 
plot(fit_rf2) 


set.seed(232)
b &lt;- 5
control_rf &lt;- trainControl(method=&quot;cv&quot;, number = b, p = .9) 
train_rf3 &lt;-  train(x_train, y_train,  
                   method = &quot;rf&quot;,  
                   ntree = 100,
                   trControl = control_rf, 
                   tuneGrid = data.frame(mtry = seq(10, 500, 20))) 
ggplot(train_rf3, highlight = TRUE)
train_rf3$bestTune
train_rf3$results$Accuracy
fit_rf3 &lt;- randomForest(x_train, y_train,  
                       mtry = train_rf3$bestTune$mtry,
                       minNode = 10) 
plot(fit_rf3) 

y_pred_rf &lt;- predict(fit_rf3, x_test, type = &quot;class&quot;)
y_pred_rf_p &lt;- predict(fit_rf3, x_test, type = &quot;prob&quot;)
confusionMatrix(y_pred_rf, y_test)
confusionMatrix(y_pred_rf, y_test)$overall[&quot;Accuracy&quot;]
F_meas(y_pred_rf, y_test, beta = 2)

roc_rf &lt;- roc(as.factor(test_set$case), y_pred_rf_p[, 2])
plot(roc_rf, print.thres=&quot;best&quot;, type = &quot;line&quot;, print.auc = TRUE, grid = TRUE, ylim = c(0,1), col = &quot;blue&quot;)</code></pre>
<pre class="r"><code>#Ensemble
p_knn &lt;- y_pred_knn_p
p_rf &lt;- y_pred_rf_p / rowSums(y_pred_rf_p) 
p_pcaknn &lt;- y_pred_pcaknn_p
p_glm &lt;- as.matrix(cbind(1-y_prob, y_prob))
colnames(p_glm) &lt;- c(0, 1)
p &lt;- (p_rf + p_knn)/2
y_pred &lt;- factor(apply(p, 1, which.max)-1) 
confusionMatrix(y_pred, y_test)
confusionMatrix(y_pred, y_test)$overall[&quot;Accuracy&quot;] 
F_meas(y_pred, y_test, beta = 2)

roc_es &lt;- roc(as.factor(test_set$case), p[, 2])
plot(roc_es, print.thres=&quot;best&quot;, type = &quot;line&quot;, print.auc = TRUE, grid = TRUE, ylim = c(0,1), col = &quot;green&quot;)</code></pre>
<pre class="r"><code>summary &lt;- data.frame()
summary[1,1] &lt;- round(confusionMatrix(y_pred_glm, y_test)$byClass[1], 3)
summary[1,2] &lt;- round(confusionMatrix(y_pred_glm, y_test)$byClass[2], 3)
summary[1,3] &lt;- round(confusionMatrix(y_pred_glm, y_test)$overall[&quot;Accuracy&quot;], 3)
summary[1,4] &lt;- round(F_meas(y_pred_glm, y_test, beta = 2), 3)
summary[1,5] &lt;- round(roc_glm$auc, 3)

summary[2,1] &lt;- round(confusionMatrix(y_pred_pcaknn, y_test)$byClass[1], 3)
summary[2,2] &lt;- round(confusionMatrix(y_pred_pcaknn, y_test)$byClass[2], 3)
summary[2,3] &lt;- round(confusionMatrix(y_pred_pcaknn, y_test)$overall[&quot;Accuracy&quot;], 3)
summary[2,4] &lt;- round(F_meas(y_pred_pcaknn, y_test, beta = 2), 3)
summary[2,5] &lt;- round(roc_pcaknn$auc, 3)

summary[3,1] &lt;- round(confusionMatrix(y_pred_knn, y_test)$byClass[1], 3)
summary[3,2] &lt;- round(confusionMatrix(y_pred_knn, y_test)$byClass[2], 3)
summary[3,3] &lt;- round(confusionMatrix(y_pred_knn, y_test)$overall[&quot;Accuracy&quot;], 3)
summary[3,4] &lt;- round(F_meas(y_pred_knn, y_test, beta = 2), 3)
summary[3,5] &lt;- round(roc_knn$auc, 3)

summary[4,1] &lt;- round(confusionMatrix(y_pred_rf, y_test)$byClass[1], 3)
summary[4,2] &lt;- round(confusionMatrix(y_pred_rf, y_test)$byClass[2], 3)
summary[4,3] &lt;- round(confusionMatrix(y_pred_rf, y_test)$overall[&quot;Accuracy&quot;], 3)
summary[4,4] &lt;- round(F_meas(y_pred_rf, y_test, beta = 2), 3)
summary[4,5] &lt;- round(roc_rf$auc, 3)

summary[5,1] &lt;- round(confusionMatrix(y_pred, y_test)$byClass[1], 3)
summary[5,2] &lt;- round(confusionMatrix(y_pred, y_test)$byClass[2], 3)
summary[5,3] &lt;- round(confusionMatrix(y_pred, y_test)$overall[&quot;Accuracy&quot;], 3)
summary[5,4] &lt;- round(F_meas(y_pred, y_test, beta = 2), 3)
summary[5,5] &lt;- round(roc_es$auc, 3)

rownames(summary) &lt;- c(&quot;PCA + GLM&quot;, &quot;PCA + KNN&quot;, &quot;KNN&quot;, &quot;Random forest&quot;, &quot;Ensemble of KNN &amp; RF&quot;)
colnames(summary) &lt;- c(&quot;Sensitivity&quot;, &quot;Specificity&quot;, &quot;Overall accuracy&quot;, &quot;F_1 score&quot;, &quot;AUC&quot;)

summary %&gt;%
  kbl() %&gt;%
  kable_material(c(&quot;striped&quot;))

ggroc(list(roc_glm, roc_pcaknn, roc_knn, roc_rf, roc_es), legacy.axes = TRUE) + 
  theme_linedraw() + 
  ggtitle(&quot;ROC&quot;) +
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color=&quot;grey&quot;, linetype=&quot;dashed&quot;) +
  scale_colour_discrete(labels = c(&quot;PCA + Logistic&quot;, &quot;PCA + KNN&quot;, &quot;KNN&quot;, &quot;Random forest&quot;, &quot;Ensemble&quot;)) +
  labs(color = &quot;Models&quot;)</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
